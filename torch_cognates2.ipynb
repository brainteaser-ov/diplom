{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, DataLoader, Dataset\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дообучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_characters_from_file(file_path):\n",
    "    chars = set()\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_number, line in enumerate(f, 1):\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:  # Проверяем, что строка содержит достаточно данных\n",
    "                word1, translation1, word2, translation2, _ = parts[:5]\n",
    "                chars.update(word1)\n",
    "                chars.update(translation1)\n",
    "                chars.update(word2)\n",
    "                chars.update(translation2)\n",
    "            else:\n",
    "                print(f\"Warning: Line {line_number} in {file_path} does not contain enough data and will be skipped.\")\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequences_from_file(file_path, tokenizer):\n",
    "    sequences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_number, line in enumerate(f, 1):\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:  # Проверяем, что строка содержит достаточно данных\n",
    "                word1, translation1, word2, translation2, _ = parts[:5]\n",
    "                seq1 = tokenizer(word1)\n",
    "                seq2 = tokenizer(translation1)\n",
    "                seq3 = tokenizer(word2)\n",
    "                seq4 = tokenizer(translation2)\n",
    "                sequences.extend([seq1, seq2, seq3, seq4])\n",
    "            else:\n",
    "                print(f\"Warning: Line {line_number} in {file_path} does not contain enough data and will be skipped.\")\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(positive_file, negative_file, tokenizer, max_seq_length, pad_index):\n",
    "    dataset = []\n",
    "    # Загрузка положительных примеров\n",
    "    with open(positive_file, 'r', encoding='utf-8') as f:\n",
    "        for line_number, line in enumerate(f, 1):\n",
    "            parts = line.strip().split('\\t')  # Разделяем строку по табуляции\n",
    "            if len(parts) >= 5:  # Проверяем, что строка содержит достаточно данных\n",
    "                word1, translation1, word2, translation2, label = parts[:5]\n",
    "                tokens1 = tokenizer(word1)\n",
    "                tokens2 = tokenizer(translation1)\n",
    "                tokens3 = tokenizer(word2)\n",
    "                tokens4 = tokenizer(translation2)\n",
    "                tokens1 = pad_or_truncate(tokens1, max_seq_length, pad_index)\n",
    "                tokens2 = pad_or_truncate(tokens2, max_seq_length, pad_index)\n",
    "                tokens3 = pad_or_truncate(tokens3, max_seq_length, pad_index)\n",
    "                tokens4 = pad_or_truncate(tokens4, max_seq_length, pad_index)\n",
    "                dataset.append({\n",
    "                    'word1': torch.tensor(tokens1, dtype=torch.long),\n",
    "                    'translation1': torch.tensor(tokens2, dtype=torch.long),\n",
    "                    'word2': torch.tensor(tokens3, dtype=torch.long),\n",
    "                    'translation2': torch.tensor(tokens4, dtype=torch.long),\n",
    "                    'label': torch.tensor(int(label), dtype=torch.float)  # Преобразуем метку в число\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Warning: Line {line_number} in {positive_file} does not contain enough data and will be skipped.\")\n",
    "    # Загрузка отрицательных примеров\n",
    "    with open(negative_file, 'r', encoding='utf-8') as f:\n",
    "        for line_number, line in enumerate(f, 1):\n",
    "            parts = line.strip().split('\\t')  \n",
    "            if len(parts) >= 5:  \n",
    "                word1, translation1, word2, translation2, label = parts[:5]\n",
    "                tokens1 = tokenizer(word1)\n",
    "                tokens2 = tokenizer(translation1)\n",
    "                tokens3 = tokenizer(word2)\n",
    "                tokens4 = tokenizer(translation2)\n",
    "                tokens1 = pad_or_truncate(tokens1, max_seq_length, pad_index)\n",
    "                tokens2 = pad_or_truncate(tokens2, max_seq_length, pad_index)\n",
    "                tokens3 = pad_or_truncate(tokens3, max_seq_length, pad_index)\n",
    "                tokens4 = pad_or_truncate(tokens4, max_seq_length, pad_index)\n",
    "                dataset.append({\n",
    "                    'word1': torch.tensor(tokens1, dtype=torch.long),\n",
    "                    'translation1': torch.tensor(tokens2, dtype=torch.long),\n",
    "                    'word2': torch.tensor(tokens3, dtype=torch.long),\n",
    "                    'translation2': torch.tensor(tokens4, dtype=torch.long),\n",
    "                    'label': torch.tensor(int(label), dtype=torch.float)  # Преобразуем метку в число\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Warning: Line {line_number} in {negative_file} does not contain enough data and will be skipped.\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_file = '/Users/oksanagoncarova/Desktop/ipynb/1/all_pairs.txt'\n",
    "negative_file = '/Users/oksanagoncarova/Desktop/ipynb/1/negative1.txt'\n",
    "test_positive = '/Users/oksanagoncarova/Desktop/ipynb/Новая папка 3/positive_test7.txt'\n",
    "test_negative = '/Users/oksanagoncarova/Desktop/ipynb/Новая папка 3/negative_test7.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка символов\n",
    "positive_chars = load_characters_from_file(positive_file)\n",
    "negative_chars = load_characters_from_file(negative_file)\n",
    "all_chars = positive_chars.union(negative_chars)\n",
    "char_list = sorted(all_chars)\n",
    "char_to_index = {char: idx + 2 for idx, char in enumerate(char_list)}  # Смещение на 2 для специальных токенов\n",
    "char_to_index['<PAD>'] = 0\n",
    "char_to_index['<UNK>'] = 1\n",
    "index_to_char = {idx: char for char, idx in char_to_index.items()}\n",
    "\n",
    "# vocab_size = len(char_to_index)\n",
    "# print(f\"Размер словаря: {vocab_size}\")\n",
    "\n",
    "# Токенизатор\n",
    "def char_tokenizer(text):\n",
    "    tokens = [char_to_index.get(char, char_to_index['<UNK>']) for char in text]\n",
    "    # print(f\"Tokenized '{text}' to {tokens}\")\n",
    "    return tokens\n",
    "\n",
    "# Загрузка последовательностей\n",
    "positive_sequences = load_sequences_from_file(positive_file, char_tokenizer)\n",
    "negative_sequences = load_sequences_from_file(negative_file, char_tokenizer)\n",
    "all_sequences = positive_sequences + negative_sequences\n",
    "\n",
    "# # Определение максимальной длины последовательности\n",
    "# sequence_lengths = [len(seq) for seq in all_sequences]\n",
    "# sequence_length = max(sequence_lengths)\n",
    "# print(f\"Максимальная длина последовательности: {sequence_length}\")\n",
    "\n",
    "# Загрузка датасета\n",
    "train_data = load_dataset(positive_file, negative_file, char_tokenizer, sequence_length, char_to_index['<PAD>'])\n",
    "random.shuffle(train_data)\n",
    "\n",
    "test_data = load_dataset(test_positive, test_negative, char_tokenizer, sequence_length, char_to_index['<PAD>'])\n",
    "random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_len):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(num_embeddings=max_len, embedding_dim=embedding_dim)\n",
    "        self.spatial_dropout = SpatialDropout1D(p=0.2)\n",
    "        self.bilstm = nn.LSTM(input_size=embedding_dim, hidden_size=64, num_layers=1,\n",
    "                              bidirectional=True, batch_first=True)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embedding_dim=128, num_heads=4, ff_dim=128, dropout=0.2)\n",
    "            for _ in range(4)\n",
    "        ])\n",
    "        self.abs_diff = AbsDiffLayer()\n",
    "        self.fc_old = nn.Sequential(\n",
    "            nn.Linear(384, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.fc_new = nn.Sequential(\n",
    "            nn.Linear(768, 384),  \n",
    "            nn.LayerNorm(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.fc_final = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, word1, translation1, word2, translation2):\n",
    "        batch_size = word1.size(0)\n",
    "        device = word1.device\n",
    "        embedded_word1 = self.embedding(word1)\n",
    "        embedded_translation1 = self.embedding(translation1)\n",
    "        positions = torch.arange(0, self.max_len, device=device).unsqueeze(0).expand(batch_size, self.max_len)\n",
    "        position_embeddings = self.position_embedding(positions)\n",
    "        embedded_word1 = embedded_word1 + position_embeddings\n",
    "        embedded_translation1 = embedded_translation1 + position_embeddings\n",
    "        embedded_word1 = self.spatial_dropout(embedded_word1)\n",
    "        embedded_translation1 = self.spatial_dropout(embedded_translation1)\n",
    "        output_word1, _ = self.bilstm(embedded_word1)\n",
    "        output_translation1, _ = self.bilstm(embedded_translation1)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            output_word1 = transformer_block(output_word1)\n",
    "            output_translation1 = transformer_block(output_translation1)\n",
    "        hidden_word1 = torch.mean(output_word1, dim=1)\n",
    "        hidden_translation1 = torch.mean(output_translation1, dim=1)\n",
    "        embedded_word2 = self.embedding(word2)\n",
    "        embedded_translation2 = self.embedding(translation2)\n",
    "        embedded_word2 = embedded_word2 + position_embeddings\n",
    "        embedded_translation2 = embedded_translation2 + position_embeddings\n",
    "        embedded_word2 = self.spatial_dropout(embedded_word2)\n",
    "        embedded_translation2 = self.spatial_dropout(embedded_translation2)\n",
    "        output_word2, _ = self.bilstm(embedded_word2)\n",
    "        output_translation2, _ = self.bilstm(embedded_translation2)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            output_word2 = transformer_block(output_word2)\n",
    "            output_translation2 = transformer_block(output_translation2)\n",
    "        hidden_word2 = torch.mean(output_word2, dim=1)\n",
    "        hidden_translation2 = torch.mean(output_translation2, dim=1)\n",
    "\n",
    "        # Объединение всех признаков\n",
    "        diff1 = self.abs_diff(hidden_word1, hidden_translation1)\n",
    "        diff2 = self.abs_diff(hidden_word2, hidden_translation2)\n",
    "        combined_features = torch.cat((hidden_word1, hidden_translation1, hidden_word2, hidden_translation2, diff1, diff2), dim=1)\n",
    "        combined_features = self.fc_new(combined_features)\n",
    "        combined_features = self.fc_old(combined_features)\n",
    "\n",
    "        # Финальный слой\n",
    "        output = self.fc_final(combined_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length: 264206\n",
      "Test data length: 11304\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train data length: {len(train_data)}\")\n",
    "print(f\"Test data length: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/xw7qvx_x0rz61xmfzfwp7b4c0000gn/T/ipykernel_33722/1647446946.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  old_model_state_dict = torch.load('model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.3609761816730428\n",
      "Epoch 2/10, Loss: 0.19763276330942614\n",
      "Epoch 3/10, Loss: 0.15640902753088765\n",
      "Epoch 4/10, Loss: 0.13594966056456986\n",
      "Epoch 5/10, Loss: 0.1215451904579878\n",
      "Epoch 6/10, Loss: 0.11457052897376335\n",
      "Epoch 7/10, Loss: 0.10246342148615625\n",
      "Epoch 8/10, Loss: 0.1016276722390248\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SiameseModel(vocab_size=2923, embedding_dim=128, max_len=43).to(device)\n",
    "\n",
    "old_model_state_dict = torch.load('model.pth')\n",
    "model.load_state_dict(old_model_state_dict, strict=False)\n",
    "\n",
    "import torch.nn.init as init\n",
    "if hasattr(model, 'fc_new'):\n",
    "    init.kaiming_normal_(model.fc_new[0].weight, mode='fan_in', nonlinearity='relu')\n",
    "    init.zeros_(model.fc_new[0].bias)\n",
    "if hasattr(model, 'fc_final'):\n",
    "    init.kaiming_normal_(model.fc_final[0].weight, mode='fan_in', nonlinearity='relu')\n",
    "    init.zeros_(model.fc_final[0].bias)\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()  \n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, device, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            word1 = batch['word1'].to(device)\n",
    "            translation1 = batch['translation1'].to(device)\n",
    "            word2 = batch['word2'].to(device)\n",
    "            translation2 = batch['translation2'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Обнуление градиентов\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Прямой проход\n",
    "            outputs = model(word1, translation1, word2, translation2)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "            # Обратный проход и обновление весов\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "# Функция для оценки модели\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            word1 = batch['word1'].to(device)\n",
    "            translation1 = batch['translation1'].to(device)\n",
    "            word2 = batch['word2'].to(device)\n",
    "            translation2 = batch['translation2'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Прямой проход\n",
    "            outputs = model(word1, translation1, word2, translation2)\n",
    "            preds = (outputs.squeeze() > 0.5).float()  \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "train_model(model, train_loader, optimizer, criterion, device, num_epochs=10)\n",
    "evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_full.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
